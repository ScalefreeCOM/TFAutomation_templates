name: 'dbt tests'

on:
  workflow_call:
    inputs:
      auto-destroy: # Destroy resources for every value except 'false' after the tests are finished
        description: 'Auto Destroy: Only "true" will prevent destruction.'
        default: 'true'
        type: string
      db_name: # Type of Database that should be tested. E.g. 'synapse', 'postgres'...
        required: true
        default: ''
        type: string
      dv4dbt-revision: # Name of the branch in datavault4dbt repository that should be tested
        required: true
        type: string
      dv4dbt-test-revision: # Name of the branch in datavault4dbt repository that should be tested
        required: true
        type: string
      dbt-test: # Input to determine if dbt tests should be executed or not
        required: true
        description: 'Skip dbt-test?'
        type: string
      macro-tests: # Input to determine if macro tests should be executed or not
        required: true
        type: string
      tech-tests:
        required: true
        type: string
      dbt_variables: # Input to determine the dbt variables for each Database
        required: true
        type: string
    outputs: #Outputs for test results for PR comment.
      dbt-test-result: 
        value: ${{ jobs.collect-job-results.outputs.dbt-test-result }}
      dbt-macro-test-result: 
        value: ${{ jobs.collect-job-results.outputs.dbt-macro-test-result }}
      tech-test-result: 
        value: ${{ jobs.collect-job-results.outputs.tech-test-result }}
      postgres-db-password:
        value: ${{ jobs.db-environments.outputs.postgres_db_password }}
      redshift-db-password:
        value: ${{ jobs.db-environments.outputs.redshift_db_password }}
      sqlserver-db-password:
        value: ${{ jobs.db-environments.outputs.sqlserver_db_password }}
    secrets:
      TFAUTOMATION_AWS_ACCESS_KEY:
        required: true
      TFAUTOMATION_AWS_SECRET_ACCESS_KEY:
        required: true
      TFAUTOMATION_AZURE_CLIENT_ID:
        required: true
      TFAUTOMATION_AZURE_TENANT_ID:
        required: true
      TFAUTOMATION_AZURE_SUBSCRIPTION_ID:
        required: true
      AWS_OIDC_ROLE:
        required: true
      DV4DBT_SA_KEYS:
        required: true
      SYNAPSE_DB_PASSWORD:
        required: true
      SNOWFLAKE_PRIVATE_KEY:
        required: true
      SNOWFLAKE_PRIVATE_KEY_dbt:
        required: true
      EXASOL_DB_PASSWORD:
        required: true
      DATABRICKS_HOST:
        required: true
      DATABRICKS_TOKEN:
        required: true
      SQLSERVER_DB_PASSWORD:
        required: false

jobs:
  db-environments:
    defaults:
      run:
        working-directory: ./db-environments/${{ inputs.db_name }} # Setting working directory for different databases
    runs-on: ubuntu-latest
    outputs: 
      # Output of IP address from Exasol EC2 instance
      public_ip: ${{ steps.tf_output_exasol.outputs.public_ip }}
      exasol_server: ${{ steps.tf_output_exasol.outputs.exasol_server }}

      # Output of Cluster- and Workspace-ID for Databricks. 
      databricks_cluster_id: ${{ steps.tf_output_databricks.outputs.databricks_cluster_id }}
      databricks_workspace_id: ${{ steps.tf_output_databricks.outputs.databricks_workspace_id }}

      #Output of database passwords                                           
      postgres_db_password: ${{ steps.tf_output_postgres.outputs.postgres_db_password }}
      redshift_db_password: ${{ steps.tf_output_redshift.outputs.redshift_db_password }}
      sqlserver_host: ${{ steps.tf_output_sqlserver.outputs.sqlserver_host }}
      sqlserver_db_password: ${{ steps.tf_output_sqlserver.outputs.sqlserver_db_password }}

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    # AWS Login
    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-region: eu-west-1
        role-to-assume: ${{ secrets.AWS_OIDC_ROLE }}

    # Microsoft Azure Login
    - name: 'Az CLI login'
      id: azure-login
      if: ${{ inputs.db_name == 'synapse' || inputs.db_name == 'fabric' }}
      uses: azure/login@v1
      with:
        client-id: ${{ secrets.TFAUTOMATION_AZURE_CLIENT_ID}}
        tenant-id: ${{ secrets.TFAUTOMATION_AZURE_TENANT_ID}}
        subscription-id: ${{ secrets.TFAUTOMATION_AZURE_SUBSCRIPTION_ID }}

    # Google Cloud Login
    - name: 'Authenticate to Google Cloud'
      if: ${{ inputs.db_name == 'bigquery' }}
      uses: 'google-github-actions/auth@v0.4.0'
      with:
        credentials_json: '${{ secrets.DV4DBT_SA_KEYS }}'
    
    # Create Token for curl API Call later. !!WORKFLOW_APP_FABRIC_SECRET expires after two years (15.11.2026) and needs to be renewed.
    - name: Get Access Token
      if: ${{inputs.db_name == 'fabric'}}
      id: get_token
      run: |
        TOKEN_RESPONSE=$(curl -X POST -H "Content-Type: application/x-www-form-urlencoded" \
        -d "grant_type=client_credentials&client_id=${{ secrets.TFAUTOMATION_AZURE_CLIENT_ID }}&client_secret=${{ secrets.WORKFLOW_APP_FABRIC_SECRET }}&resource=https://management.azure.com/" \
        https://login.microsoftonline.com/${{ secrets.TFAUTOMATION_AZURE_TENANT_ID }}/oauth2/token)
        echo "ACCESS_TOKEN=$(echo $TOKEN_RESPONSE | jq -r .access_token)" >> $GITHUB_ENV

    # Setting terraform input variables for different Database Environments.
    - name: Create terraform.tfvars for synapse
      if: ${{ inputs.db_name == 'synapse' }}
      run: |
          cd $GITHUB_WORKSPACE/db-environments/synapse/ && cat << EOF > terraform.tfvars
          sql_administrator_login_password="${{ secrets.SYNAPSE_DB_PASSWORD }}"
          EOF
    - name: create terraform.tfvars for fabric
      if: ${{ inputs.db_name == 'fabric' }}
      run: |
          cd $GITHUB_WORKSPACE/db-environments/fabric/ && cat << EOF > terraform.tfvars
          connectionStringID="${{ secrets.FABRIC_SQL_ENDPOINT_START }}"
          warehouseName="${{ vars.FABRIC_DB }}"
          EOF
    - name: Increment active count in DynamoDB and start Capacity
      if: ${{ inputs.db_name == 'fabric' }}
      run: |
          # Increment the semaphore
          aws dynamodb update-item \
            --table-name dv4dbt-cicd-semaphore \
            --key '{"ID": {"S": "capacity"}}' \
            --update-expression "SET active_count = active_count + :incr" \
            --expression-attribute-values '{":incr": {"N": "1"}}' \
            --return-values "UPDATED_NEW" > output.json
          
          # Extract the value of semaphore
          ACTIVE_COUNT=$(cat output.json | jq -r '.Attributes.active_count.N')

          # Start capacity if semaphore is 1 after incrementing.
          if [ "$ACTIVE_COUNT" -eq "1" ]; then
            echo "Starting capacity as this is the first active run."
            curl -X POST \
              -H "Authorization: Bearer $ACCESS_TOKEN" \
              -H "Content-Type: application/json" \
              -H "Content-Length: 0" \
              "https://management.azure.com/subscriptions/${{ secrets.TFAUTOMATION_AZURE_SUBSCRIPTION_ID }}/resourceGroups/dv4dbt_db_environment/providers/Microsoft.Fabric/capacities/dv4dbtcicd/resume?api-version=2023-11-01"
            # Wait for 25 seconds to allow the capacity to start.
            echo "Waiting 25 seconds for capacity to start..."
            sleep 25
          else
            echo "Capacity is already running with $ACTIVE_COUNT active runs."
          fi
          
    - name: Create key file for snowflake
      if: ${{ inputs.db_name == 'snowflake' }}
      run: |
          cd $GITHUB_WORKSPACE/db-environments/generated_files/ && cat << EOF > snowflake_authenticator_key.pem
          ${{ secrets.SNOWFLAKE_PRIVATE_KEY }}
          EOF
    - name: Create terraform.tfvars for databricks
      if: ${{ inputs.db_name == 'databricks' }}
      run: |
          cd $GITHUB_WORKSPACE/db-environments/databricks/ && cat << EOF > terraform.tfvars
          token="${{ secrets.DATABRICKS_TOKEN }}"
          host="${{ secrets.DATABRICKS_HOST}}"
          EOF

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v2
      with:
        terraform_version: 1.8.0
        terraform_wrapper: false

    # Initialize Terraform backend dynamically with run_id for each database.
    - name: Terraform Init
      if: ${{ inputs.db_name != 'oracle' }}
      run: terraform init -backend-config "key=datavault4dbtenvironments/${{ github.run_id }}/${{ inputs.db_name }}/terraform.state" -backend-config "bucket=scalefree-tf-backend" -backend-config "region=eu-west-1" -backend-config "dynamodb_table=scalefree-tf-lock-backend"
        
    # Apply Terraform configuration with run_id as input
    - name: Terraform Apply
      if: ${{ inputs.db_name != 'oracle' && inputs.db_name != 'fabric' }}
      run: terraform apply -auto-approve -var="run_id=${{ github.run_id }}"

       # Apply Terraform configuration with run_id as input for fabric to avoid fail massages (check if its possible to use another deployment method for fabric)
    - name: Terraform Apply Fabric
      if: ${{ inputs.db_name == 'fabric' }}
      continue-on-error: true
      run: terraform apply -auto-approve -var="run_id=${{ github.run_id }}"

    # Store Run_id in FIFO SQS when the environment should not be destroyed, to shut down the environment automatically at the End of the day.
    - name: Send message to FIFO SQS
      if: ${{ inputs.auto-destroy == 'false' }}
      run: |
        aws sqs send-message --queue-url ${{ secrets.FIFO_SQS_QUEUE_URL }} --message-body "${{ github.run_id }}/${{ inputs.db_name }}" --message-group-id ${{ inputs.db_name }} --message-deduplication-id ${{ inputs.db_name }}

    # Giving the IP Adress of exasol EC2 instance as output from Terraform to GitHub. 
    - name: Terraform Output exasol
      if: ${{ inputs.db_name == 'exasol' }}
      id: tf_output_exasol
      run: |
         echo "public_ip=$(terraform output -json | jq -r .publicIP.value)" >> $GITHUB_OUTPUT
         echo "exasol_server=$(terraform output -json | jq -r .exasol_server.value)" >> $GITHUB_OUTPUT
         sleep 30

    # Giving the Cluster- and Workspace-ID of Databricks environment from Terraform to GitHub.
    - name: Terraform Output databricks
      if: ${{ inputs.db_name == 'databricks' }}
      id: tf_output_databricks
      run: |
        echo "databricks_cluster_id=$(terraform output -json | jq -r .databricks_cluster_id.value)" >> $GITHUB_OUTPUT
        echo "databricks_workspace_id=$(terraform output -json | jq -r .databricks_workspace_id.value)" >> $GITHUB_OUTPUT
        

    - name: Ensure Oracle RDS is running
      if: ${{ inputs.db_name == 'oracle' }}
      run: |
        set -e

        DB_INSTANCE_ID="datavault4dbtrdsoracle-static"
        REGION="${{ env.AWS_REGION }}"

        STATUS=$(aws rds describe-db-instances \
          --db-instance-identifier "$DB_INSTANCE_ID" \
          --region "$REGION" \
          --query 'DBInstances[0].DBInstanceStatus' \
          --output text)

        echo "Current RDS status: $STATUS"

        if [ "$STATUS" = "stopped" ]; then
          echo "Starting RDS instance..."
          aws rds start-db-instance \
            --db-instance-identifier "$DB_INSTANCE_ID" \
            --region "$REGION"
        elif [ "$STATUS" = "stopping" ]; then
          echo "RDS instance is stopping. Waiting until it is fully stopped..."
          while [ "$STATUS" = "stopping" ]; do
            sleep 30
            STATUS=$(aws rds describe-db-instances \
              --db-instance-identifier "$DB_INSTANCE_ID" \
              --region "$REGION" \
              --query 'DBInstances[0].DBInstanceStatus' \
              --output text)
            echo "Current RDS status: $STATUS"
          done

          if [ "$STATUS" = "stopped" ]; then
            echo "Instance is now stopped. Starting RDS instance..."
            aws rds start-db-instance \
              --db-instance-identifier "$DB_INSTANCE_ID" \
              --region "$REGION"
          else
            echo "Unexpected RDS status after waiting: $STATUS. Exiting."
            exit 1
          fi
        else
          echo "RDS instance is already in '$STATUS' state. No need to start."
          fi

        echo "Waiting for RDS to become available..."
        aws rds wait db-instance-available \
          --db-instance-identifier "$DB_INSTANCE_ID" \
          --region "$REGION"

        echo "ORACLE_USER=DV4DBTSCHEMA" >> $GITHUB_ENV
        echo "ORACLE_PASSWORD=${{ secrets.ORACLE_STATIC_PW }}" >> $GITHUB_ENV



    - name: Increment Oracle semaphore
      if: ${{ inputs.db_name == 'oracle' }}
      run: |
        aws dynamodb update-item \
          --table-name dv4dbt-cicd-semaphore \
          --key '{"ID": {"S": "oracle_rds"}}' \
          --update-expression "SET active_count = if_not_exists(active_count, :zero) + :incr" \
          --expression-attribute-values '{":incr": {"N": "1"}, ":zero": {"N": "0"}}'
    

    - name: Wait for Oracle RDS to be available                                                          
      if: ${{ inputs.db_name == 'oracle' }}
      run: |
        aws rds wait db-instance-available \
        --db-instance-identifier datavault4dbtrdsoracle-static \
        --region ${{ env.AWS_REGION }}
   

    - name: Terraform Output postgres
      if: ${{ inputs.db_name == 'postgres'}}
      id: tf_output_postgres
      run: |
        echo "postgres_db_password=$(terraform output -json | jq -r .postgres_db_password.value)" >> $GITHUB_OUTPUT
    
    - name: Terraform Output redshift 
      if: ${{ inputs.db_name == 'redshift'}}
      id: tf_output_redshift
      run: |
        echo "redshift_db_password=$(terraform output -json | jq -r .redshift_db_password.value)" >> "$GITHUB_OUTPUT"
    
    - name: Terraform Output sqlserver
      if: ${{ inputs.db_name == 'sqlserver' }}
      id: tf_output_sqlserver
      run: |
        echo "sqlserver_host=$(terraform output -json | jq -r .sqlserver_host.value)" >> $GITHUB_OUTPUT
        echo "sqlserver_db_password=$(terraform output -json | jq -r .sqlserver_password.value)" >> $GITHUB_OUTPUT

    # Setup and starting of Exasol environment.
    - name: Setup environment and start Server
      if: ${{ inputs.db_name == 'exasol' }}
      uses: appleboy/ssh-action@v1.0.3
      with:
          host: ${{ steps.tf_output_exasol.outputs.PUBLIC_IP }}
          username: ${{ secrets.SSH_USER_EXASOL }}
          key: ${{ secrets.SSH_PRIVATE_KEY_EXASOL }}
          script: |
              docker run --name dv4dbt-exasol --network host --detach --privileged --stop-timeout 120  exasol/docker-db:8.27.0
  generate-files: # generating and uploading of files needed to connect to the environments.
    needs: db-environments
    runs-on: ubuntu-latest
    if: success() || failure() 
    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Create dv4dbt-test-sa.json
        run: |
            cd $GITHUB_WORKSPACE/db-environments/generated_files/ && cat << EOF > dv4dbt-sa-keys.json
            ${{ secrets.DV4DBT_SA_KEYS }}
            EOF
  
      - name: Create profiles.yml
        run: |
            cd $GITHUB_WORKSPACE/db-environments/generated_files/ && cat << EOF > profiles.yml
            snowflake:
              target: user_pw
              outputs:
                user_pw:
                  type: snowflake
                  user: ${{ vars.SNOWFLAKE_USERNAME }}
                  private_key: ${{ secrets.SNOWFLAKE_PRIVATE_KEY_DBT }} 
                  password: ${{ secrets.SNOWFLAKE_PASSWORD }}
                  account: ${{ vars.SNOWFLAKE_ACCOUNT }}
                  role: ${{ vars.SNOWFLAKE_ROLE }}
                  database: ${{ vars.SNOWFLAKE_DB }}
                  warehouse: ${{ vars.SNOWFLAKE_WAREHOUSE }}
                  schema: ${{ vars.SNOWFLAKE_SCHEMA }}${{ github.run_id }}
                  threads: ${{ vars.SNOWFLAKE_THREADS }}
            bigquery:      
              target: keyfile
              outputs:
                keyfile:
                  type: bigquery
                  method: service-account
                  project: ${{ vars.BIGQUERY_PROJECT }}
                  dataset: ${{ vars.BIGQUERY_DATASET }}${{ github.run_id }}
                  threads: ${{ vars.BIGQUERY_THREADS }}
                  keyfile: /user/app/profiles/dv4dbt-sa-keys.json
            redshift:
              target: user_pw
              outputs:
                user_pw:
                  type: redshift
                  host: ${{ vars.REDSHIFT_DB_HOST_START }}${{ github.run_id }}${{ vars.REDSHIFT_DB_HOST_END }}
                  user: ${{ vars.REDSHIFT_DB_USERNAME }}
                  password: ${{ needs.db-environments.outputs.redshift_db_password }}
                  dbname: ${{ vars.REDSHIFT_DB }}${{ github.run_id }}
                  schema: ${{ vars.REDSHIFT_DB_SCHEMA }}${{ github.run_id }}
                  port: ${{ vars.REDSHIFT_PORT }}
                  threads: ${{ vars.REDSHIFT_THREADS }}
            postgres:
              target: user_pw
              outputs:
                user_pw:
                  type: postgres
                  host: ${{ vars.POSTGRES_DB_HOST_START }}${{ github.run_id }}${{vars.POSTGRES_DB_HOST_END}}
                  user: ${{ vars.POSTGRES_DB_USERNAME }}
                  password: ${{ needs.db-environments.outputs.postgres_db_password }}
                  port: ${{ vars.POSTGRES_PORT }}
                  dbname: ${{ vars.POSTGRES_DB }}${{ github.run_id }}
                  schema: ${{ vars.POSTGRES_DB_SCHEMA }}
                  threads: ${{ vars.POSTGRES_DB_THREADS }}
            synapse:
              target: user_pw
              outputs:
                user_pw:
                  type: synapse
                  driver: ${{ vars.SYNAPSE_DRIVER }}
                  server: ${{ vars.SYNAPSE_SQL_ENDPOINT }}
                  port: ${{ vars.SYNAPSE_PORT }}
                  database: ${{ vars.SYNAPSE_DB }}${{ github.run_id }}
                  schema: ${{ vars.SYNAPSE_DB_SCHEMA }}
                  authentication: ServicePrincipal
                  tenant_id: ${{ secrets.TFAUTOMATION_AZURE_TENANT_ID}}
                  client_id: ${{ secrets.TFAUTOMATION_AZURE_CLIENT_ID}}
                  client_secret: ${{ secrets.AZURE_CLIENT_SECRET}}
                  threads: ${{ vars.SYNAPSE_THREADS }}
                  username: ${{ vars.SYNAPSE_DB_USERNAME }}
                  password: ${{ secrets.SYNAPSE_DB_PASSWORD }}
            sqlserver:
              target: user_pw
              outputs:
                user_pw:
                  type: sqlserver
                  driver: 'ODBC Driver 18 for SQL Server'
                  server: ${{ needs.db-environments.outputs.sqlserver_host }}
                  port: 1433
                  database: master
                  schema: dbo
                  user: adminuser 
                  password: ${{ needs.db-environments.outputs.sqlserver_db_password }}
                  encrypt: true
                  trust_cert: true
                  threads: 4
            exasol:
              target: user_pw
              outputs:
                user_pw:
                  type: exasol
                  threads: ${{ vars.EXASOL_THREADS }}
                  dsn: ${{ needs.db-environments.outputs.public_ip}}:${{ vars.EXASOL_PORT }}
                  user: ${{ vars.EXASOL_DB_USERNAME }}
                  password: ${{ secrets.EXASOL_DB_PASSWORD }}
                  server: ${{ steps.tf_output_exasol.outputs.exasol_server }}
                  dbname: ${{ vars.EXASOL_DB }}
                  schema: ${{ vars.EXASOL_DB_SCHEMA }}
                  query_timeout: 60
                  connection_timeout: 60
                  socket_timeout: 60
            fabric:
              target: user_pw
              outputs:
                user_pw:
                  type: fabric
                  driver: ${{ vars.FABRIC_DRIVER }}
                  server: ${{ secrets.FABRIC_SQL_ENDPOINT_START }}${{ vars.FABRIC_SQL_ENDPOINT_END }}
                  port: ${{ vars.FABRIC_PORT }}
                  database: ${{ vars.FABRIC_DB }}
                  schema: ${{ vars.FABRIC_SCHEMA }}${{ github.run_id }}
                  authentication: ServicePrincipal
                  tenant_id: ${{ secrets.TFAUTOMATION_AZURE_TENANT_ID}}
                  client_id: ${{ secrets.TFAUTOMATION_AZURE_CLIENT_ID}}
                  client_secret: ${{ secrets.AZURE_CLIENT_SECRET}}
                  threads: ${{ vars.FABRIC_THREADS }}
            databricks:
              target: user_pw
              outputs:
                user_pw:
                  type: databricks
                  schema: ${{ vars.DATABICKS_SCHEMA }}${{ github.run_id }}
                  host: ${{ secrets.DATABRICKS_HOST }}
                  http_path: sql/protocolv1/o/${{ needs.db-environments.outputs.databricks_workspace_id }}/${{ needs.db-environments.outputs.databricks_cluster_id }}
                  token: ${{ secrets.DATABRICKS_TOKEN }}
                  threads: ${{ vars.DATABICKS_THREADS }}
                  port: ${{ vars.DATABRICKS_PORT }}
            oracle:
              target: user_pw
              outputs:
                user_pw:
                  type: oracle
                  user: ${{ vars.ORACLE_DB_USER }}
                  pass: ${{ secrets.ORACLE_STATIC_PW }}                                                         
                  protocol: ${{ vars.ORACLE_DB_PROTOCOL }}
                  host: ${{ vars.ORACLE_STATIC_HOST }}
                  port: ${{ vars.ORACLE_DB_PORT }}
                  service: ${{ vars.ORACLE_DB_SERVICE }}
                  database: ${{ vars.ORACLE_DB }}
                  schema: ${{ vars.ORACLE_DB_SCHEMA }}
                  threads: ${{ vars.ORACLE_DB_THREADS }}
            EOF
            
      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with: 
          name: my-files-${{ inputs.db_name }}
          path: |
            db-environments/generated_files/dv4dbt-sa-keys.json
            db-environments/generated_files/profiles.yml

  execute-seeds:
    needs: [generate-files, db-environments]
    runs-on: ubuntu-latest
    if: ${{ (success() || failure()) && (inputs.tech-tests == 'yes' || inputs.dbt-test == 'yes' || inputs.macro-tests == 'yes' )}}
    steps:
    - name: Checkout CICD Repo
      uses: actions/checkout@v4

    - name: Generate token
      id: generate_token
      uses: actions/create-github-app-token@v1
      with:
        app-id: ${{ secrets.WORKFLOW_APP_ID }}
        private-key: ${{ secrets.WORKFLOW_APP_PRIVATE_KEY }}
        owner: ${{ github.repository_owner }}
        repositories: "datavault4dbt-automatic-tests"

    - name: Checkout datavault4dbt-automatic-tests  # Access repo to retrieve the config file
      uses: actions/checkout@v4
      with:
        repository: ScalefreeCOM/datavault4dbt-automatic-tests  
        token: ${{ steps.generate_token.outputs.token }}
        path: datavault4dbt-automatic-tests
        ref: '${{ inputs.test_branch }}'

    - uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    # Downloading generated artifacts like profiles.yml
    - name: Download Artifacts 
      uses: actions/download-artifact@v4
      with: 
        name: my-files-${{ inputs.db_name }}
        path: db-environments/generated_files

    # Waiting for Exasol to start the server.
    - name: Wait 60s (Exasol only)
      if: ${{ inputs.db_name == 'exasol' }}
      run: sleep 60

    - name: Wait for BigQuery dataset to initialize
      if: ${{ inputs.db_name == 'bigquery' }}
      run: sleep 60
  

    # Building the docker Image from the docker Files for each database.
    - name: Build ${{ inputs.db_name }} image
      run: docker build -t dv4dbt-${{ inputs.db_name }} --build-arg DBT_DV4DBT_REVISION=${{ inputs.dv4dbt-revision }} --build-arg DBT_TEST_REVISION=${{ inputs.dv4dbt-test-revision }} --build-arg SSH_PRIVATE_REPO_KEY="${{ secrets.SSH_PRIVATE_REPO_KEY }}" -f $GITHUB_WORKSPACE/db-environments/${{ inputs.db_name }}/Dockerfile .

    - name: Run ${{ inputs.db_name }} container dbt seed
      if: success() || failure()
      run: docker run --name dv4dbt-seed-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt seed --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      
  tech-tests:
    needs: [generate-files, execute-seeds, db-environments]
    runs-on: ubuntu-latest
    # if success or failure to always execute except in the case of an abortion.
    if: ${{ (success() || failure()) && inputs.tech-tests == 'yes' }}


    steps:
    - name: Checkout CICD Repo
      uses: actions/checkout@v4

    - name: Generate token
      id: generate_token
      uses: actions/create-github-app-token@v1
      with:
        app-id: ${{ secrets.WORKFLOW_APP_ID }}
        private-key: ${{ secrets.WORKFLOW_APP_PRIVATE_KEY }}
        owner: ${{ github.repository_owner }}
        repositories: "datavault4dbt-automatic-tests"

    - name: Checkout datavault4dbt-automatic-tests  # Access repo to retrieve the config file
      uses: actions/checkout@v4
      with:
        repository: ScalefreeCOM/datavault4dbt-automatic-tests  
        token: ${{ steps.generate_token.outputs.token }}
        path: datavault4dbt-automatic-tests
        ref: '${{ inputs.test_branch }}'

    - uses: actions/setup-python@v4
      with:
        python-version: '3.x'

    # Downloading generated artifacts like profiles.yml
    - name: Download Artifacts 
      uses: actions/download-artifact@v4
      with: 
        name: my-files-${{ inputs.db_name }}
        path: db-environments/generated_files

    # Waiting for Exasol to start the server.
    - name: Wait 60s (Exasol only)
      if: ${{ inputs.db_name == 'exasol' }}
      run: sleep 60
      
    - name: Wait 60s 
      if: ${{ inputs.db_name == 'bigquery' }}
      run: sleep 60
    
    # Building the docker Image from the docker Files for each database.
    - name: Build ${{ inputs.db_name }} image
      run: docker build -t dv4dbt-${{ inputs.db_name }} --build-arg DBT_DV4DBT_REVISION=${{ inputs.dv4dbt-revision }} --build-arg DBT_TEST_REVISION=${{ inputs.dv4dbt-test-revision }} --build-arg SSH_PRIVATE_REPO_KEY="${{ secrets.SSH_PRIVATE_REPO_KEY }}" -f $GITHUB_WORKSPACE/db-environments/${{ inputs.db_name }}/Dockerfile .

    - name: install python packages
      run: |
        python -m pip install --upgrade pip
        pip install -r ./datavault4dbt-automatic-tests/db-environments/techtest/requirements.txt

    - name: (debug) List Files
      run: ls -laR | grep -v "./.git"
      
    - name: Compare Bigquery
      if: ${{ inputs.db_name == 'bigquery' }}
      run: |
        python ./datavault4dbt-automatic-tests/db-environments/techtest/testBigQuery.py -i './datavault4dbt-automatic-tests/seeds' -s './datavault4dbt-automatic-tests/db-environments/techtest/schema' -p ${{ vars.BIGQUERY_PROJECT }} -d ${{ vars.BIGQUERY_DATASET }}${{ github.run_id }} -k $GITHUB_WORKSPACE/db-environments/generated_files/dv4dbt-sa-keys.json
    
    - name: Compare Exasol
      if: ${{ inputs.db_name == 'exasol' }}
      run: |
        python ./datavault4dbt-automatic-tests/db-environments/techtest/testExasol.py -i './datavault4dbt-automatic-tests/seeds' -s './datavault4dbt-automatic-tests/db-environments/techtest/schema' -ho "${{ needs.db-environments.outputs.public_ip}}" -p "${{ vars.EXASOL_PORT }}" -u "${{ vars.EXASOL_DB_USERNAME }}" -psw "${{ secrets.EXASOL_DB_PASSWORD }}" -sch "${{ vars.EXASOL_DB_SCHEMA }}"
       
    - name: Compare Postgres
      if: ${{ inputs.db_name == 'postgres' }}
      run: |
        python ./datavault4dbt-automatic-tests/db-environments/techtest/testPostgres.py -i './datavault4dbt-automatic-tests/seeds' -s './datavault4dbt-automatic-tests/db-environments/techtest/schema' -ho ${{ vars.POSTGRES_DB_HOST_START }}${{ github.run_id }}${{vars.POSTGRES_DB_HOST_END}} -p ${{ vars.POSTGRES_PORT }} -u ${{ vars.POSTGRES_DB_USERNAME }} -psw ${{ needs.db-environments.outputs.postgres_db_password }} -db ${{ vars.POSTGRES_DB }}${{ github.run_id }} -sch ${{ vars.POSTGRES_DB_SCHEMA }}

    - name: Compare Redshift
      if: ${{ inputs.db_name == 'redshift' }}
      run: |
        python ./datavault4dbt-automatic-tests/db-environments/techtest/testRedshift.py -i './datavault4dbt-automatic-tests/seeds' -s './datavault4dbt-automatic-tests/db-environments/techtest/schema' -ho ${{ vars.REDSHIFT_DB_HOST_START }}${{ github.run_id }}${{ vars.REDSHIFT_DB_HOST_END }} -p ${{ vars.REDSHIFT_PORT }} -u ${{ vars.REDSHIFT_DB_USERNAME }} -psw ${{ needs.db-environments.outputs.redshift_db_password }} -db ${{ vars.REDSHIFT_DB }}${{ github.run_id }} -sch ${{ vars.REDSHIFT_DB_SCHEMA }}${{ github.run_id }}
 
    - name: Install ODBC Driver for SQL Server (Synapse)
      if: ${{ inputs.db_name == 'synapse' }}
      run: |
        sudo apt-get update
        curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg
        sudo install -o root -g root -m 644 microsoft.gpg /etc/apt/trusted.gpg.d/
        curl https://packages.microsoft.com/config/ubuntu/22.04/prod.list | sudo tee /etc/apt/sources.list.d/mssql-release.list
        sudo apt-get update
        sudo ACCEPT_EULA=Y apt-get install -y msodbcsql17
        sudo apt-get install -y unixodbc-dev


            
    - name: Compare Synapse
      if: ${{ inputs.db_name == 'synapse' }}
      run: | 
        python ./datavault4dbt-automatic-tests/db-environments/techtest/testSynapse.py -i './datavault4dbt-automatic-tests/seeds' -s './datavault4dbt-automatic-tests/db-environments/techtest/schema' -ho "${{ vars.SYNAPSE_SQL_ENDPOINT }}" -u "${{ vars.SYNAPSE_DB_USERNAME }}" -psw "${{ secrets.SYNAPSE_DB_PASSWORD }}" -db "${{ vars.SYNAPSE_DB }}${{ github.run_id }}" -sch "${{ vars.SYNAPSE_DB_SCHEMA }}"
    - name: Save private key from secret
      run: |
         echo "${{ secrets.SNOWFLAKE_PRIVATE_KEY_NOT_ENCRYPTED }}" | sed 's/\\n/\n/g' > /tmp/snowflake_key.p8  
         
    - name: Compare Snowflake
      if: ${{ inputs.db_name == 'snowflake' }}
      run: |
        python ./datavault4dbt-automatic-tests/db-environments/techtest/testSnowflake.py -i './datavault4dbt-automatic-tests/seeds' -s './datavault4dbt-automatic-tests/db-environments/techtest/schema' -a ${{ vars.SNOWFLAKE_ACCOUNT }} -w ${{ vars.SNOWFLAKE_WAREHOUSE }} -u ${{ vars.SNOWFLAKE_USERNAME }} -pk /tmp/snowflake_key.p8 -db ${{ vars.SNOWFLAKE_DB }} -sch ${{ vars.SNOWFLAKE_SCHEMA }}${{ github.run_id }} -r ${{ vars.SNOWFLAKE_ROLE }}
  
    - name: Compare Databricks
      if: ${{ inputs.db_name == 'databricks' }}
      run: |
        python ./datavault4dbt-automatic-tests/db-environments/techtest/testDatabricks.py -i './datavault4dbt-automatic-tests/seeds' -s './datavault4dbt-automatic-tests/db-environments/techtest/schema' -ho ${{ secrets.DATABRICKS_HOST }} -p ${{ vars.DATABRICKS_PORT }} -path sql/protocolv1/o/${{ needs.db-environments.outputs.databricks_workspace_id }}/${{ needs.db-environments.outputs.databricks_cluster_id }} -t ${{ secrets.DATABRICKS_TOKEN }} -sch  ${{ vars.DATABICKS_SCHEMA }}${{ github.run_id }} 
        
    - name: Install ODBC Driver for SQL Server (Fabric)
      if: ${{ inputs.db_name == 'fabric' }}
      run: |
        sudo apt-get update
        curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg
        sudo install -o root -g root -m 644 microsoft.gpg /etc/apt/trusted.gpg.d/
        curl https://packages.microsoft.com/config/ubuntu/22.04/prod.list | sudo tee /etc/apt/sources.list.d/mssql-release.list
        sudo apt-get update
        sudo ACCEPT_EULA=Y apt-get install -y msodbcsql17 msodbcsql18
        sudo apt-get install -y unixodbc-dev

    - name: Compare Fabric
      if: ${{ inputs.db_name == 'fabric' }}
      run: |
        python ./datavault4dbt-automatic-tests/db-environments/techtest/testFabrics.py -i './datavault4dbt-automatic-tests/seeds' -s './datavault4dbt-automatic-tests/db-environments/techtest/schema' -ho ${{ secrets.FABRIC_SQL_ENDPOINT_START }}${{ vars.FABRIC_SQL_ENDPOINT_END }} -c ${{ secrets.TFAUTOMATION_AZURE_CLIENT_ID}} -cs ${{ secrets.AZURE_CLIENT_SECRET}} -tid  ${{ secrets.TFAUTOMATION_AZURE_TENANT_ID}} -db ${{ vars.FABRIC_DB }} -sch ${{ vars.FABRIC_SCHEMA }}${{ github.run_id }}
    
    - name: Install ODBC Driver for SQL Server (SQL Server)
      if: ${{ inputs.db_name == 'sqlserver' }}
      run: |
        sudo apt-get update
        curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > microsoft.gpg
        sudo install -o root -g root -m 644 microsoft.gpg /etc/apt/trusted.gpg.d/
        curl https://packages.microsoft.com/config/ubuntu/22.04/prod.list | sudo tee /etc/apt/sources.list.d/mssql-release.list
        sudo apt-get update
        sudo ACCEPT_EULA=Y apt-get install -y msodbcsql18
        sudo apt-get install -y unixodbc-dev
    
    - name: Compare SQL Server
      if: ${{ inputs.db_name == 'sqlserver' }}
      run: |
        python ./datavault4dbt-automatic-tests/db-environments/techtest/testSqlServer.py \
        -i './datavault4dbt-automatic-tests/seeds' \
        -s './datavault4dbt-automatic-tests/db-environments/techtest/schema' \
        -ho ${{ needs.db-environments.outputs.sqlserver_host }} \
        -p 1433 \
        -db master \
        -u adminuser \
        -psw ${{ needs.db-environments.outputs.sqlserver_db_password }} \
        -sch dbo

    - name: Compare Oracle
      if: ${{ inputs.db_name == 'oracle' }}
      run: |
        python ./datavault4dbt-automatic-tests/db-environments/techtest/testOracle.py -i './datavault4dbt-automatic-tests/seeds' -s './datavault4dbt-automatic-tests/db-environments/techtest/schema' -ho ${{ vars.ORACLE_STATIC_HOST }} -p ${{ vars.ORACLE_DB_PORT }} -sv ${{ vars.ORACLE_DB_SERVICE }} -db ${{ vars.ORACLE_DB }} -u ${{ vars.ORACLE_DB_USER }} -psw ${{ secrets.ORACLE_STATIC_PW }} -sch ${{ vars.ORACLE_DB_SCHEMA }}

  dbt-tests:
    needs: [generate-files, execute-seeds]
    runs-on: ubuntu-latest
    # if success or failure to always execute except in the case of an abortion.
    if: ${{ (success() || failure()) && inputs.dbt-test == 'yes' }}


    steps:
    - name: Checkout
      uses: actions/checkout@v3

    # Downloading generated artifacts like profiles.yml
    - name: Download Artifacts 
      uses: actions/download-artifact@v4
      with: 
        name: my-files-${{ inputs.db_name }}
        path: db-environments/generated_files

    # Waiting for Exasol to start the server.
    - name: Wait 60s (Exasol only)
      if: ${{ inputs.db_name == 'exasol' }}
      run: sleep 60

    # Building the docker Image from the docker Files for each database.
    - name: Build ${{ inputs.db_name }} image
      run: docker build -t dv4dbt-${{ inputs.db_name }} --build-arg DBT_DV4DBT_REVISION=${{ inputs.dv4dbt-revision }} --build-arg DBT_TEST_REVISION=${{ inputs.dv4dbt-test-revision }} --build-arg SSH_PRIVATE_REPO_KEY="${{ secrets.SSH_PRIVATE_REPO_KEY }}" -f $GITHUB_WORKSPACE/db-environments/${{ inputs.db_name }}/Dockerfile .
  
    # Waking up the fabric database (Could cause problems because of simultanious tech-tests. dbt seed when tests there are already running)
    - name: Fabric warm up SQL Pool
      if: ${{ inputs.db_name == 'fabric' && (success() || failure())}}
      continue-on-error: true
      run: |
        docker run --name dv4dbt-${{ inputs.db_name }}-warm-up --network host -e DBT_INCREMENTAL_RUN=0 dv4dbt-${{ inputs.db_name }} dbt seed --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }} 
    
    # Ensuring Fabric SQL pool is warmed up
    - name: Wait 42s (Fabric only)
      if: ${{ inputs.db_name == 'fabric' && (success() || failure())}}
      run: sleep 42
    
    # Running all Tests with different Variables.
    - name: Run ${{ inputs.db_name }} container DBT_INCREMENTAL_RUN=0 Variables set to True
      if: success() || failure()
      run: |
        docker run --name dv4dbt-v1-i0-${{ inputs.db_name }} --network host -e DBT_INCREMENTAL_RUN=0 dv4dbt-${{ inputs.db_name }} dbt build --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }} -f --vars '{${{inputs.dbt_variables }}, datavault4dbt.copy_rsrc_ldts_input_columns: True, datavault4dbt.hashkey_input_case_sensitive: True, datavault4dbt.hashdiff_input_case_sensitive: True, datavault4dbt.include_business_objects_before_appearance: True}'
    - name: Run ${{ inputs.db_name }} container DBT_INCREMENTAL_RUN=1 Variables set to True
      if: success() || failure()
      run: |
        docker run --name dv4dbt-v1-i1-${{ inputs.db_name }} --network host -e DBT_INCREMENTAL_RUN=1 dv4dbt-${{ inputs.db_name }} dbt build --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }} --vars '{${{inputs.dbt_variables }}, datavault4dbt.copy_rsrc_ldts_input_columns: True, datavault4dbt.hashkey_input_case_sensitive: True, datavault4dbt.hashdiff_input_case_sensitive: True, datavault4dbt.include_business_objects_before_appearance: True}'

    - name: Run ${{ inputs.db_name }} container DBT_INCREMENTAL_RUN=0 Variables set to False
      if: success() || failure()
      run: |
        docker run --name dv4dbt-v2-i0-${{ inputs.db_name }} --network host -e DBT_INCREMENTAL_RUN=0 dv4dbt-${{ inputs.db_name }} dbt build --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }} -f --vars '{${{ inputs.dbt_variables }}, datavault4dbt.copy_rsrc_ldts_input_columns: False, datavault4dbt.hashkey_input_case_sensitive: False, datavault4dbt.hashdiff_input_case_sensitive: False, datavault4dbt.include_business_objects_before_appearance: False}'
    
    # For Fabric the schema needs to be empty after the last test. In that case, we use an extra variable to delete every object in the schema after the tests.
    # Also the Databricks schema is not managed in terraform so it will be deleted through dbt with an extra variable for databricks.
    # DBT tests checking if it is a fabric environment as well. 
    - name: Run ${{ inputs.db_name }} container DBT_INCREMENTAL_RUN=1 Variables set to False and empty schema
      if: success() || failure()
      run: |
        docker run --name dv4dbt-v2-i1-${{ inputs.db_name }} --network host -e DBT_EMPTY_FABRIC_SCHEMA_ON_RUN_END=1 -e DBT_DROP_DATABRICKS_SCHEMA_ON_RUN_END=1 -e DBT_INCREMENTAL_RUN=1 dv4dbt-${{ inputs.db_name }} dbt build --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }} --vars '{${{ inputs.dbt_variables }}, datavault4dbt.copy_rsrc_ldts_input_columns: False, datavault4dbt.hashkey_input_case_sensitive: False, datavault4dbt.hashdiff_input_case_sensitive: False, datavault4dbt.include_business_objects_before_appearance: False}'


  dbt-macro-tests:
      needs: [generate-files, execute-seeds]
      runs-on: ubuntu-latest
      if: ${{ (success() || failure()) && inputs.macro-tests == 'yes' }}
          
      steps:
      - name: Checkout
        uses: actions/checkout@v3

      # Downloading generated artifacts like profile.yml
      - name: Download Artifacts
        uses: actions/download-artifact@v4
        with: 
          name: my-files-${{ inputs.db_name }}
          path: db-environments/generated_files

      # Waiting for the Exasol server to start.
      - name: Wait 60s
        if: ${{ inputs.db_name == 'exasol' }}
        run: sleep 60
     
      # Building docker image from dockerfile for each database.
      - name: Build ${{ inputs.db_name }} image
        run: docker build -t dv4dbt-${{ inputs.db_name }} --build-arg DBT_DV4DBT_REVISION=${{ inputs.dv4dbt-revision }} --build-arg DBT_TEST_REVISION=${{ inputs.dv4dbt-test-revision }} --build-arg SSH_PRIVATE_REPO_KEY="${{ secrets.SSH_PRIVATE_REPO_KEY }}" -f $GITHUB_WORKSPACE/db-environments/${{ inputs.db_name }}/Dockerfile .
      
      # Run hardcoded Macro tests.
      - name: Run run-operation is_list --args '{obj}' in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-01-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation is_list --args '{obj}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation is_nothing --args '{obj}' in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-02-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation is_nothing --args '{obj}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation is_something --args '{obj}' in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-03-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation is_something --args '{obj}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation is_expression --args '{obj}' in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-04-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation is_expression --args '{obj}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation prepend_generated_by in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-06-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation prepend_generated_by -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation replace_standard --args '{input_variable, global_variable, default_value}' in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-07-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation replace_standard --args '{input_variable, global_variable, default_value}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation process_columns_to_select in ${{ inputs.db_name }} container
        if: success() || failure()
        run: |
           docker run --name dv4dbt-macro-test-08-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation process_columns_to_select --args '{"columns_list": [], "exclude_columns_list": []}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation extract_column_names in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-09-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation extract_column_names -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation extract_input_columns in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-10-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation extract_input_columns -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation print_list --args '{list_to_print: abc}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-11-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation print_list --args '{list_to_print: abc}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation alias_all --args '{columns: col}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-12-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation alias_all --args '{columns: col}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation alias --args '{alias_config: abc}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-13-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation alias --args '{alias_config: abc}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation as_constant --args '{column_str: abc}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-14-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation as_constant --args '{column_str: abc}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation check_required_parameters in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-15-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation check_required_parameters -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation concat_ws --args '{string_list: abc}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-16-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation concat_ws --args '{string_list: abc}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation escape_column_names in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-17-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation escape_column_names -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation expand_column_list --args '{columns: [abc]}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-18-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation expand_column_list --args '{columns: [abc]}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation multikey in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-19-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation multikey -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation hash_columns in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-20-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation hash_columns -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation beginning_of_all_times in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-21-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation beginning_of_all_times -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation current_timestamp in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-22-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation current_timestamp -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation current_timestamp_in_utc in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-23-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation current_timestamp_in_utc -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation type_timestamp in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-24-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation type_timestamp -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation end_of_all_times in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-25-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation end_of_all_times -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation generate_schema_name in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-26-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation generate_schema_name -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      
      # Run different tests for Oracle and every other database, because of different syntax in Oracle.
      - name: "Run run-operation get_query_results_as_dict --args '{query: select 1}' in ${{ inputs.db_name }} container"
        if: ${{ (success() || failure()) && inputs.db_name != 'oracle' }}
        run: |
          docker run --name dv4dbt-macro-test-27-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation get_query_results_as_dict --args '{query: select 1}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation get_query_results_as_dict --args '{query: select 1 from dual}' in ${{ inputs.db_name }} container"
        if: ${{ (success() || failure()) && inputs.db_name == 'oracle' }}
        run: |
          docker run --name dv4dbt-macro-test-27-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation get_query_results_as_dict --args '{query: select 1 from dual}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      
      - name: "Run run-operation get_standard_string --args '{string_list: abc}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-28-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation get_standard_string --args '{string_list: abc}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation ghost_record_per_datatype --args '{column_name: col, datatype: string, ghost_record_type: unknown}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-29-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation ghost_record_per_datatype --args '{column_name: col, datatype: string, ghost_record_type: unknown}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation hash_default_values in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-30-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation hash_default_values -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation hash_method in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-31-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation hash_method -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation attribute_standardise in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-32-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation attribute_standardise -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation hash --args '{columns: abc, alias: def}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-33-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation hash --args '{columns: abc, alias: def}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation limit_rows in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-34-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation limit_rows -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation max_datetime in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-35-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation max_datetime -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation prefix --args '{columns: abc, prefix_str: def}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-38-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation prefix --args '{columns: abc, prefix_str: def}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation source_model_processing --args '{source_models: abc}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-39-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation source_model_processing --args '{source_models: abc}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation string_to_timestamp in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-40-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation string_to_timestamp -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation timestamp_format in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-41-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation timestamp_format -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
     
  # trigger the destruction of environments if set as a variable during call, after all tests are done.
  trigger-destroy-environments:
    if: ${{ always() && inputs.auto-destroy != 'false' }} 
    needs: [ dbt-tests, dbt-macro-tests, tech-tests ]

  # Same setup jobs as in the setup of the environments.
    defaults:
      run:
        working-directory: ./db-environments/${{ inputs.db_name }}

    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Configure AWS Credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-region: eu-west-1
        role-to-assume: ${{ secrets.AWS_OIDC_ROLE }}

    - name: 'Az CLI login'
      if: ${{ inputs.db_name == 'synapse' || inputs.db_name == 'fabric' }}
      uses: azure/login@v1
      with:
        client-id: ${{ secrets.TFAUTOMATION_AZURE_CLIENT_ID}}
        tenant-id: ${{ secrets.TFAUTOMATION_AZURE_TENANT_ID}}
        subscription-id: ${{ secrets.TFAUTOMATION_AZURE_SUBSCRIPTION_ID }}

    - name: 'Authenticate to Google Cloud'
      if: ${{ inputs.db_name == 'bigquery' }}
      uses: 'google-github-actions/auth@v0.4.0'
      with:
        credentials_json: '${{ secrets.DV4DBT_SA_KEYS }}'

    - name: Create terraform.tfvars for synapse
      if: ${{ inputs.db_name == 'synapse' }}
      run: |
          cd $GITHUB_WORKSPACE/db-environments/synapse/ && cat << EOF > terraform.tfvars
          sql_administrator_login_password="${{ secrets.SYNAPSE_DB_PASSWORD }}"
          EOF
    - name: Create terraform.tfvars for databricks
      if: ${{ inputs.db_name == 'databricks' }}
      run: |
          cd $GITHUB_WORKSPACE/db-environments/databricks/ && cat << EOF > terraform.tfvars
          token="${{ secrets.DATABRICKS_TOKEN }}"
          host="${{ secrets.DATABRICKS_HOST}}"
          EOF
    - name: Create key file for snowflake
      if: ${{ inputs.db_name == 'snowflake' }}
      run: |
          cd $GITHUB_WORKSPACE/db-environments/generated_files/ && cat << EOF > snowflake_authenticator_key.pem
          ${{ secrets.SNOWFLAKE_PRIVATE_KEY }}
          EOF

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v2
      with:
        terraform_version: 1.8.0
    
    - name: Terraform Init
      run: terraform init -backend-config "key=datavault4dbtenvironments/${{ github.run_id }}/${{ inputs.db_name }}/terraform.state" -backend-config "bucket=scalefree-tf-backend" -backend-config "region=eu-west-1" -backend-config "dynamodb_table=scalefree-tf-lock-backend"
        
    # destruction of created terraform ressources.
    - name: Terraform Destroy
      run: terraform destroy -auto-approve -var="run_id=${{ github.run_id }}"

    - name: Decrement Oracle semaphore and stop RDS if unused
      if: ${{ inputs.db_name == 'oracle' }}
      run: |
        ACTIVE_COUNT=$(aws dynamodb get-item \
          --table-name dv4dbt-cicd-semaphore \
          --key '{"ID": {"S": "oracle_rds"}}' \
          --query 'Item.active_count.N' \
          --output text 2>/dev/null || echo "0")

        echo "Current Oracle semaphore count: $ACTIVE_COUNT"

        if [ "$ACTIVE_COUNT" -gt "0" ]; then
          NEW_COUNT=$((ACTIVE_COUNT - 1))
          aws dynamodb update-item \
            --table-name dv4dbt-cicd-semaphore \
            --key '{"ID": {"S": "oracle_rds"}}' \
            --update-expression "SET active_count = :new" \
            --expression-attribute-values "{\":new\": {\"N\": \"$NEW_COUNT\"}}"

          echo "Decremented. New count: $NEW_COUNT"

          if [ "$NEW_COUNT" -eq "0" ]; then
            echo "Stopping Oracle RDS..."
            aws rds stop-db-instance \
              --db-instance-identifier datavault4dbtrdsoracle-static \
              --region ${{ env.AWS_REGION }}
          else
            echo "Other workflows still using Oracle RDS."
          fi
        else
          echo "Semaphore already 0. Skipping."
        fi

    # Create a token for curl API Call later. !!WORKFLOW_APP_FABRIC_SECRET expires after two years (15.11.2026) and needs to be renewed.
    - name: Get Access Token
      if: ${{ inputs.db_name == 'fabric' }}
      id: get_token
      run: |
        TOKEN_RESPONSE=$(curl -X POST -H "Content-Type: application/x-www-form-urlencoded" \
        -d "grant_type=client_credentials&client_id=${{ secrets.TFAUTOMATION_AZURE_CLIENT_ID }}&client_secret=${{ secrets.WORKFLOW_APP_FABRIC_SECRET }}&resource=https://management.azure.com/" \
        https://login.microsoftonline.com/${{ secrets.TFAUTOMATION_AZURE_TENANT_ID }}/oauth2/token)
        echo "ACCESS_TOKEN=$(echo $TOKEN_RESPONSE | jq -r .access_token)" >> $GITHUB_ENV

    - name: Decrement active count in DynamoDB and stop Capacity
      if: ${{ inputs.db_name == 'fabric' }}
      run: |
          # Decrement semaphore and get the new Value
          aws dynamodb update-item \
            --table-name dv4dbt-cicd-semaphore \
            --key '{"ID": {"S": "capacity"}}' \
            --update-expression "SET active_count = active_count - :decr" \
            --expression-attribute-values '{":decr": {"N": "1"}}' \
            --return-values "UPDATED_NEW" > output.json

          # Get new value of active_count (semaphore) 
          ACTIVE_COUNT=$(cat output.json | jq -r '.Attributes.active_count.N')

          # If semaphore = 0, capacity will be paused
          if [ "$ACTIVE_COUNT" -eq "0" ]; then
            echo "Pausing capacity as no active runs are left."
             curl -X POST \
              -H "Authorization: Bearer $ACCESS_TOKEN" \
              -H "Content-Type: application/json" \
              -H "Content-Length: 0" \
              "https://management.azure.com/subscriptions/${{ secrets.TFAUTOMATION_AZURE_SUBSCRIPTION_ID }}/resourceGroups/dv4dbt_db_environment/providers/Microsoft.Fabric/capacities/dv4dbtcicd/suspend?api-version=2023-11-01"
          else
            echo "Remaining active runs: $ACTIVE_COUNT. Capacity will not be paused."
          fi

    # Job to send the results of tests for better monitoring.
  collect-job-results:
    if: always()
    needs: [ dbt-tests, dbt-macro-tests, tech-tests ]

    runs-on: ubuntu-latest
    outputs: # defining outputs
      dbt-test-result: ${{ steps.results.outputs.dbt-test-result}}
      dbt-macro-test-result: ${{ steps.results.outputs.dbt-macro-test-result}}
      tech-test-result: ${{ steps.results.outputs.tech-test-result}}

    steps:
    - name: Collect results
      id: results
      # setting output values.
      run: |
        echo "dbt-test-result=${{ needs.dbt-tests.result }}" >> $GITHUB_OUTPUT
        echo "dbt-macro-test-result=${{ needs.dbt-macro-tests.result }}" >> $GITHUB_OUTPUT
        echo "dbt-macro-test-result=${{ needs.tech-tests.result }}" >> $GITHUB_OUTPUT
