name: 'dbt tests'

on:
  workflow_call:
    inputs:
      auto-destroy: # Destroy resources for every value except 'false' after the tests are finished
        description: 'Auto Destroy: Only "true" will prevent destruction.'
        default: 'true'
        type: string
      db_name: # Type of Database that should be tested. E.g. 'synapse', 'postgres'...
        required: true
        default: ''
        type: string
      dv4dbt-revision: # Name of the branch in datavault4dbt repository  that should be tested
        required: true
        type: string
      macro-tests: # Input to determine if macro tests should be executed or not
        required: true
        type: boolean
    outputs: #Outputs for test results for PR comment.
      dbt-test-result: 
        value: ${{ jobs.collect-job-results.outputs.dbt-test-result }}
      dbt-macro-test-result: 
        value: ${{ jobs.collect-job-results.outputs.dbt-macro-test-result }}
      oracle-db-password:
        value: ${{ jobs.db-environments.outputs.oracle_db_password }}
      postgres-db-password:
        value: ${{ jobs.db-environments.outputs.postgres_db_password }}
      redshift-db-password:
        value: ${{ jobs.db-environments.outputs.redshift_db_password }}
    secrets:
      TFAUTOMATION_AWS_ACCESS_KEY:
        required: true
      TFAUTOMATION_AWS_SECRET_ACCESS_KEY:
        required: true
      TFAUTOMATION_AZURE_CLIENT_ID:
        required: true
      TFAUTOMATION_AZURE_TENANT_ID:
        required: true
      TFAUTOMATION_AZURE_SUBSCRIPTION_ID:
        required: true
      DV4DBT_SA_KEYS:
        required: true
      SYNAPSE_DB_PASSWORD:
        required: true
      SNOWFLAKE_PRIVATE_KEY:
        required: true
      SNOWFLAKE_PRIVATE_KEY_dbt:
        required: true
      EXASOL_DB_PASSWORD:
        required: true
      DATABRICKS_HOST:
        required: true
      DATABRICKS_TOKEN:
        required: true

jobs:
  db-environments:
    defaults:
      run:
        working-directory: ./db-environments/${{ inputs.db_name }} # Setting working directory for different databases
    runs-on: ubuntu-latest
    outputs: 
      # Output of IP address from Exasol EC2 instance
      public_ip: ${{ steps.tf_output_exasol.outputs.public_ip }}

      # Output of Cluster- and Workspace-ID for Databricks. 
      databricks_cluster_id: ${{ steps.tf_output_databricks.outputs.databricks_cluster_id }}
      databricks_workspace_id: ${{ steps.tf_output_databricks.outputs.databricks_workspace_id }}

      #Output of database passwords
      oracle_db_password: ${{ steps.tf_output_oracle.outputs.oracle_db_password }}
      postgres_db_password: ${{ steps.tf_output_postgres.outputs.postgres_db_password }}
      redshift_db_password: ${{ steps.tf_output_redshift.outputs.redshift_db_password }}

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    # AWS Login
    - name: Configure AWS Credentials Action For GitHub Actions
      uses: aws-actions/configure-aws-credentials@v1-node16
      with:
        aws-access-key-id:  ${{ secrets.TFAUTOMATION_AWS_ACCESS_KEY }}
        aws-secret-access-key:  ${{ secrets.TFAUTOMATION_AWS_SECRET_ACCESS_KEY }}
        aws-region: eu-west-1

    # Microsoft Azure Login
    - name: 'Az CLI login'
      id: azure-login
      if: ${{ inputs.db_name == 'synapse' || inputs.db_name == 'fabric'}}
      uses: azure/login@v1
      with:
        client-id: ${{ secrets.TFAUTOMATION_AZURE_CLIENT_ID}}
        tenant-id: ${{ secrets.TFAUTOMATION_AZURE_TENANT_ID}}
        subscription-id: ${{ secrets.TFAUTOMATION_AZURE_SUBSCRIPTION_ID }}

    # Google Cloud Login
    - name: 'Authenticate to Google Cloud'
      if: ${{ inputs.db_name == 'bigquery' }}
      uses: 'google-github-actions/auth@v0.4.0'
      with:
        credentials_json: '${{ secrets.DV4DBT_SA_KEYS }}'
    
    # Create Token for curl API Call later. !!WORKFLOW_APP_FABRIC_SECRET expires after two years (15.11.2026) and needs to be renewed.
    - name: Get Access Token
      if: ${{inputs.db_name == 'fabric'}}
      id: get_token
      run: |
        TOKEN_RESPONSE=$(curl -X POST -H "Content-Type: application/x-www-form-urlencoded" \
        -d "grant_type=client_credentials&client_id=${{ secrets.TFAUTOMATION_AZURE_CLIENT_ID }}&client_secret=${{ secrets.WORKFLOW_APP_FABRIC_SECRET }}&resource=https://management.azure.com/" \
        https://login.microsoftonline.com/${{ secrets.TFAUTOMATION_AZURE_TENANT_ID }}/oauth2/token)
        echo "ACCESS_TOKEN=$(echo $TOKEN_RESPONSE | jq -r .access_token)" >> $GITHUB_ENV

    # Setting terraform input variables for different Database Environments.
    - name: Create terraform.tfvars for synapse
      if: ${{ inputs.db_name == 'synapse' }}
      run: |
          cd $GITHUB_WORKSPACE/db-environments/synapse/ && cat << EOF > terraform.tfvars
          sql_administrator_login_password="${{ secrets.SYNAPSE_DB_PASSWORD }}"
          EOF
    - name: create terraform.tfvars for fabric
      if: ${{ inputs.db_name == 'fabric' }}
      run: |
          cd $GITHUB_WORKSPACE/db-environments/fabric/ && cat << EOF > terraform.tfvars
          connectionStringID="${{ secrets.FABRIC_SQL_ENDPOINT_START }}"
          warehouseName="${{ vars.FABRIC_DB }}"
          EOF
    - name: Increment active count in DynamoDB and start Capacity
      if: ${{ inputs.db_name == 'fabric' }}
      run: |
          # Increment the semaphore
          aws dynamodb update-item \
            --table-name dv4dbt-cicd-semaphore \
            --key '{"ID": {"S": "capacity"}}' \
            --update-expression "SET active_count = active_count + :incr" \
            --expression-attribute-values '{":incr": {"N": "1"}}' \
            --return-values "UPDATED_NEW" > output.json
          
          # Extract the value of semaphore
          ACTIVE_COUNT=$(cat output.json | jq -r '.Attributes.active_count.N')

          # Start capacity if semaphore is 1 after incrementing.
          if [ "$ACTIVE_COUNT" -eq "1" ]; then
            echo "Starting capacity as this is the first active run."
            curl -X POST \
              -H "Authorization: Bearer $ACCESS_TOKEN" \
              -H "Content-Type: application/json" \
              -H "Content-Length: 0" \
              "https://management.azure.com/subscriptions/${{ secrets.TFAUTOMATION_AZURE_SUBSCRIPTION_ID }}/resourceGroups/dv4dbt_db_environment/providers/Microsoft.Fabric/capacities/dv4dbtcicd/resume?api-version=2023-11-01"
            # Wait for 25 seconds to allow the capacity to start.
            echo "Waiting 25 seconds for capacity to start..."
            sleep 25
          else
            echo "Capacity is already running with $ACTIVE_COUNT active runs."
          fi
          
    - name: Create terraform.tfvars for snowflake
      if: ${{ inputs.db_name == 'snowflake' }}
      run: |
          cd $GITHUB_WORKSPACE/db-environments/generated_files/ && cat << EOF > snowflake_authenticator_key.pem
          ${{ secrets.SNOWFLAKE_PRIVATE_KEY }}
          EOF
    - name: Create terraform.tfvars for databricks
      if: ${{ inputs.db_name == 'databricks' }}
      run: |
          cd $GITHUB_WORKSPACE/db-environments/databricks/ && cat << EOF > terraform.tfvars
          token="${{ secrets.DATABRICKS_TOKEN }}"
          host="${{ secrets.DATABRICKS_HOST}}"
          EOF

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v2
      with:
        terraform_version: 1.8.0
        terraform_wrapper: false
    
    # Initialize Terraform backend dynamically with run_id for each database.
    - name: Terraform Init
      run: terraform init -backend-config "key=datavault4dbtenvironments/${{ github.run_id }}/${{ inputs.db_name }}/terraform.state" -backend-config "bucket=scalefree-tf-backend" -backend-config "region=eu-west-1" -backend-config "dynamodb_table=scalefree-tf-lock-backend"
        
    # Apply Terraform configuration with run_id as input
    - name: Terraform Apply
      run: terraform apply -auto-approve -var="run_id=${{ github.run_id }}"

    # Store Run_id in FIFO SQS when the environment should not be destroyed, to shut down the environment automatically at the End of the day.
    - name: Send message to FIFO SQS
      if: ${{ inputs.auto-destroy == 'false' }}
      run: |
        aws sqs send-message --queue-url ${{ secrets.FIFO_SQS_QUEUE_URL }} --message-body "${{ github.run_id }}/${{ inputs.db_name }}" --message-group-id ${{ inputs.db_name }} --message-deduplication-id ${{ inputs.db_name }}

    # Giving the IP Adress of exasol EC2 instance as output from Terraform to GitHub. 
    - name: Terraform Output exasol
      if: ${{ inputs.db_name == 'exasol' }}
      id: tf_output_exasol
      run: |
        echo "public_ip=$(terraform output -json | jq -r .publicIP.value)" >> $GITHUB_OUTPUT
        sleep 30

    # Giving the Cluster- and Workspace-ID of Databricks environment from Terraform to GitHub.
    - name: Terraform Output databricks
      if: ${{ inputs.db_name == 'databricks' }}
      id: tf_output_databricks
      run: |
        echo "databricks_cluster_id=$(terraform output -json | jq -r .databricks_cluster_id.value)" >> $GITHUB_OUTPUT
        echo "databricks_workspace_id=$(terraform output -json | jq -r .databricks_workspace_id.value)" >> $GITHUB_OUTPUT

    - name: Terraform Output oracle
      if: ${{ inputs.db_name == 'oracle'}}
      id: tf_output_oracle
      run: |
        echo "oracle_db_password=$(terraform output -json | jq -r .oracle_db_password.value)" >> $GITHUB_OUTPUT

    - name: Terraform Output postgres
      if: ${{ inputs.db_name == 'postgres'}}
      id: tf_output_postgres
      run: |
        echo "postgres_db_password=$(terraform output -json | jq -r .postgres_db_password.value)" >> $GITHUB_OUTPUT
    
    - name: Terraform Output redshift 
      if: ${{ inputs.db_name == 'redshift'}}
      id: tf_output_redshift
      run: |
        echo "redshift_db_password=$(terraform output -json | jq -r .redshift_db_password.value)" >> $GITHUB_OUTPUT

    # Setup and starting of Exasol environment.
    - name: Setup environment and start Server
      if: ${{ inputs.db_name == 'exasol' }}
      uses: appleboy/ssh-action@v1.0.3
      with:
          host: ${{ steps.tf_output.outputs.PUBLIC_IP }}
          username: ${{ secrets.SSH_USER_EXASOL }}
          key: ${{ secrets.SSH_PRIVATE_KEY_EXASOL }}
          script: |
              docker run --name dv4dbt-exasol --network host --detach --privileged --stop-timeout 120  exasol/docker-db:8.27.0
  generate-files: # generating and uploading of files needed to connect to the environments.
    needs: db-environments
    runs-on: ubuntu-latest
    if: success() || failure() 
    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Create dv4dbt-test-sa.json
        run: |
            cd $GITHUB_WORKSPACE/db-environments/generated_files/ && cat << EOF > dv4dbt-sa-keys.json
            ${{ secrets.DV4DBT_SA_KEYS }}
            EOF
  
      - name: Create profiles.yml
        run: |
            cd $GITHUB_WORKSPACE/db-environments/generated_files/ && cat << EOF > profiles.yml
            snowflake:
              target: user_pw
              outputs:
                user_pw:
                  type: snowflake
                  user: ${{ vars.SNOWFLAKE_USERNAME }}
                  private_key: ${{ secrets.SNOWFLAKE_PRIVATE_KEY_DBT }}                
                  account: ${{ vars.SNOWFLAKE_ACCOUNT }}
                  role: ${{ vars.SNOWFLAKE_ROLE }}
                  database: ${{ vars.SNOWFLAKE_DB }}
                  warehouse: ${{ vars.SNOWFLAKE_WAREHOUSE }}
                  schema: ${{ vars.SNOWFLAKE_SCHEMA }}${{ github.run_id }}
                  threads: ${{ vars.SNOWFLAKE_THREADS }}
            bigquery:      
              target: keyfile
              outputs:
                keyfile:
                  type: bigquery
                  method: service-account
                  project: ${{ vars.BIGQUERY_PROJECT }}
                  dataset: ${{ vars.BIGQUERY_DATASET }}${{ github.run_id }}
                  threads: ${{ vars.BIGQUERY_THREADS }}
                  keyfile: /user/app/profiles/dv4dbt-sa-keys.json
            redshift:
              target: user_pw
              outputs:
                user_pw:
                  type: redshift
                  host: ${{ vars.REDSHIFT_DB_HOST_START }}${{ github.run_id }}${{ vars.REDSHIFT_DB_HOST_END }}
                  user: ${{ vars.REDSHIFT_DB_USERNAME }}
                  password: ${{ needs.db-environments.outputs.redshift_db_password }}
                  dbname: ${{ vars.REDSHIFT_DB }}${{ github.run_id }}
                  schema: ${{ vars.REDSHIFT_DB_SCHEMA }}${{ github.run_id }}
                  port: ${{ vars.REDSHIFT_PORT }}
                  threads: ${{ vars.REDSHIFT_THREADS }}
            postgres:
              target: user_pw
              outputs:
                user_pw:
                  type: postgres
                  host: ${{ vars.POSTGRES_DB_HOST_START }}${{ github.run_id }}${{vars.POSTGRES_DB_HOST_END}}
                  user: ${{ vars.POSTGRES_DB_USERNAME }}
                  password: '${{ needs.db-environments.outputs.postgres_db_password }}'
                  port: ${{ vars.POSTGRES_PORT }}
                  dbname: ${{ vars.POSTGRES_DB }}${{ github.run_id }}
                  schema: ${{ vars.POSTGRES_DB_SCHEMA }}
                  threads: ${{ vars.POSTGRES_DB_THREADS }}
            synapse:
              target: user_pw
              outputs:
                user_pw:
                  type: synapse
                  driver: ${{ vars.SYNAPSE_DRIVER }}
                  server: ${{ vars.SYNAPSE_SQL_ENDPOINT }}
                  port: ${{ vars.SYNAPSE_PORT }}
                  database: ${{ vars.SYNAPSE_DB }}${{ github.run_id }}
                  schema: ${{ vars.SYNAPSE_DB_SCHEMA }}
                  authentication: ServicePrincipal
                  tenant_id: ${{ secrets.TFAUTOMATION_AZURE_TENANT_ID}}
                  client_id: ${{ secrets.TFAUTOMATION_AZURE_CLIENT_ID}}
                  client_secret: ${{ secrets.AZURE_CLIENT_SECRET}}
                  threads: ${{ vars.SYNAPSE_THREADS }}
            exasol:
              target: user_pw
              outputs:
                user_pw:
                  type: exasol
                  threads: ${{ vars.EXASOL_THREADS }}
                  dsn: ${{ needs.db-environments.outputs.public_ip}}:${{ vars.EXASOL_PORT }}
                  user: ${{ vars.EXASOL_DB_USERNAME }}
                  password: ${{ secrets.EXASOL_DB_PASSWORD }}
                  dbname: ${{ vars.EXASOL_DB }}
                  schema: ${{ vars.EXASOL_DB_SCHEMA }}
                  query_timeout: 60
                  connection_timeout: 60
                  socket_timeout: 60
            fabric:
              target: user_pw
              outputs:
                user_pw:
                  type: fabric
                  driver: ${{ vars.FABRIC_DRIVER }}
                  server: ${{ secrets.FABRIC_SQL_ENDPOINT_START }}${{ vars.FABRIC_SQL_ENDPOINT_END }}
                  port: ${{ vars.FABRIC_PORT }}
                  database: ${{ vars.FABRIC_DB }}
                  schema: ${{ vars.FABRIC_SCHEMA }}${{ github.run_id }}
                  authentication: ServicePrincipal
                  tenant_id: ${{ secrets.TFAUTOMATION_AZURE_TENANT_ID}}
                  client_id: ${{ secrets.TFAUTOMATION_AZURE_CLIENT_ID}}
                  client_secret: ${{ secrets.AZURE_CLIENT_SECRET}}
                  threads: ${{ vars.FABRIC_THREADS }}
            databricks:
              target: dev
              outputs:
                dev:
                  type: databricks
                  schema: ${{ vars.DATABICKS_SCHEMA }}${{ github.run_id }}
                  host: ${{ secrets.DATABRICKS_HOST }}
                  http_path: sql/protocolv1/o/${{ needs.db-environments.outputs.databricks_workspace_id }}/${{ needs.db-environments.outputs.databricks_cluster_id }}
                  token: ${{ secrets.DATABRICKS_TOKEN }}
                  threads: ${{ vars.DATABICKS_THREADS }}
            oracle:
              target: user_pw
              outputs:
                user_pw:
                  type: oracle
                  user: ${{ vars.ORACLE_DB_USER }}
                  pass: ${{ needs.db-environments.outputs.oracle_db_password }}
                  protocol: ${{ vars.ORACLE_DB_PROTOCOL }}
                  host: ${{ vars.ORACLE_DB_HOST_START }}${{ github.run_id }}${{ vars.ORACLE_DB_HOST_END }}
                  port: ${{ vars.ORACLE_DB_PORT }}
                  service: ${{ vars.ORACLE_DB_SERVICE }}
                  database: ${{ vars.ORACLE_DB }}
                  schema: ${{ vars.ORACLE_DB_SCHEMA }}
                  threads: ${{ vars.ORACLE_DB_THREADS }}
            EOF
      - name: Upload Artifacts
        uses: actions/upload-artifact@v3
        with: 
          name: my-files
          path: |
            db-environments/generated_files/dv4dbt-sa-keys.json
            db-environments/generated_files/profiles.yml
      
  
  dbt-tests:
    needs: generate-files
    runs-on: ubuntu-latest
    # if success or failure to always execute except in the case of an abortion.
    if: success() || failure()

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    # Downloading generated artifacts like profiles.yml
    - name: Download Artifacts 
      uses: actions/download-artifact@v3
      with: 
        name: my-files
        path: db-environments/generated_files

    # Waiting for Exasol to start the server.
    - name: Wait 60s
      if: ${{ inputs.db_name == 'exasol' }}
      run: sleep 60

    # Building the docker Image from the docker Files for each database.
    - name: Build ${{ inputs.db_name }} image
      run: docker build -t dv4dbt-${{ inputs.db_name }} --build-arg DBT_DV4DBT_REVISION=${{ inputs.dv4dbt-revision }} --build-arg SSH_PRIVATE_REPO_KEY="${{ secrets.SSH_PRIVATE_REPO_KEY }}" -f $GITHUB_WORKSPACE/db-environments/${{ inputs.db_name }}/Dockerfile .
    
    # Running all Tests with different Variables.
    - name: Run ${{ inputs.db_name }} container DBT_INCREMENTAL_RUN=0 Variables set to True
      if: success() || failure()
      run: |
        docker run --name dv4dbt-v1-i0-${{ inputs.db_name }} --network host -e DBT_INCREMENTAL_RUN=0 dv4dbt-${{ inputs.db_name }} dbt build --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }} -f --vars '{datavault4dbt.set_casing: "lowercase", datavault4dbt.copy_rsrc_ldts_input_columns: True, datavault4dbt.hashkey_input_case_sensitive: True, datavault4dbt.hashdiff_input_case_sensitive: True, datavault4dbt.include_business_objects_before_appearance: True}'
    - name: Run ${{ inputs.db_name }} container DBT_INCREMENTAL_RUN=1 Variables set to True
      if: success() || failure()
      run: |
        docker run --name dv4dbt-v1-i1-${{ inputs.db_name }} --network host -e DBT_INCREMENTAL_RUN=1 dv4dbt-${{ inputs.db_name }} dbt build --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }} --vars '{datavault4dbt.set_casing: "lowercase", datavault4dbt.copy_rsrc_ldts_input_columns: True, datavault4dbt.hashkey_input_case_sensitive: True, datavault4dbt.hashdiff_input_case_sensitive: True, datavault4dbt.include_business_objects_before_appearance: True}'

    - name: Run ${{ inputs.db_name }} container DBT_INCREMENTAL_RUN=0 Variables set to False
      if: success() || failure()
      run: |
        docker run --name dv4dbt-v2-i0-${{ inputs.db_name }} --network host -e DBT_INCREMENTAL_RUN=0 dv4dbt-${{ inputs.db_name }} dbt build --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }} -f --vars '{datavault4dbt.set_casing: "lowercase", datavault4dbt.copy_rsrc_ldts_input_columns: False, datavault4dbt.hashkey_input_case_sensitive: False, datavault4dbt.hashdiff_input_case_sensitive: False, datavault4dbt.include_business_objects_before_appearance: False}'
    
    # For Fabric the schema needs to be empty after the last test. In that case, we use an extra variable to delete every object in the schema after the tests.
    # DBT tests checking if it is a fabric environment as well. 
    - name: Run ${{ inputs.db_name }} container DBT_INCREMENTAL_RUN=1 Variables set to False and empty schema
      if: success() || failure()
      run: |
        docker run --name dv4dbt-v2-i1-${{ inputs.db_name }} --network host -e DBT_EMPTY_FABRIC_SCHEMA_ON_RUN_END=1 -e DBT_INCREMENTAL_RUN=1 dv4dbt-${{ inputs.db_name }} dbt build --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }} --vars '{datavault4dbt.set_casing: "lowercase", datavault4dbt.copy_rsrc_ldts_input_columns: False, datavault4dbt.hashkey_input_case_sensitive: False, datavault4dbt.hashdiff_input_case_sensitive: False, datavault4dbt.include_business_objects_before_appearance: False}'


  dbt-macro-tests:
      needs: generate-files
      runs-on: ubuntu-latest
      if: ${{ (success() || failure()) && inputs.macro-tests }}
          
      steps:
      - name: Checkout
        uses: actions/checkout@v3

      # Downloading generated artifacts like profile.yml
      - name: Download Artifacts
        uses: actions/download-artifact@v3
        with: 
          name: my-files
          path: db-environments/generated_files

      # Waiting for the Exasol server to start.
      - name: Wait 60s
        if: ${{ inputs.db_name == 'exasol' }}
        run: sleep 60

      # Building docker image from dockerfile for each database.
      - name: Build ${{ inputs.db_name }} image
        run: docker build -t dv4dbt-${{ inputs.db_name }} --build-arg DBT_DV4DBT_REVISION=${{ inputs.dv4dbt-revision }} --build-arg SSH_PRIVATE_REPO_KEY="${{ secrets.SSH_PRIVATE_REPO_KEY }}" -f $GITHUB_WORKSPACE/db-environments/${{ inputs.db_name }}/Dockerfile .
      
      # Run hardcoded Macro tests.
      - name: Run run-operation is_list --args '{obj}' in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-01-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation is_list --args '{obj}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation is_nothing --args '{obj}' in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-02-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation is_nothing --args '{obj}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation is_something --args '{obj}' in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-03-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation is_something --args '{obj}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation is_expression --args '{obj}' in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-04-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation is_expression --args '{obj}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation prepend_generated_by in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-06-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation prepend_generated_by -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation replace_standard --args '{input_variable, global_variable, default_value}' in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-07-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation replace_standard --args '{input_variable, global_variable, default_value}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation process_columns_to_select in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-08-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation process_columns_to_select -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation extract_column_names in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-09-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation extract_column_names -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation extract_input_columns in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-10-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation extract_input_columns -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation print_list --args '{list_to_print: abc}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-11-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation print_list --args '{list_to_print: abc}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation alias_all --args '{columns: col}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-12-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation alias_all --args '{columns: col}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation alias --args '{alias_config: abc}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-13-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation alias --args '{alias_config: abc}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation as_constant --args '{column_str: abc}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-14-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation as_constant --args '{column_str: abc}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation check_required_parameters in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-15-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation check_required_parameters -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation concat_ws --args '{string_list: abc}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-16-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation concat_ws --args '{string_list: abc}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation escape_column_names in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-17-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation escape_column_names -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation expand_column_list --args '{columns: [abc]}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-18-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation expand_column_list --args '{columns: [abc]}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation multikey in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-19-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation multikey -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation hash_columns in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-20-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation hash_columns -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation beginning_of_all_times in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-21-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation beginning_of_all_times -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation current_timestamp in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-22-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation current_timestamp -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation current_timestamp_in_utc in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-23-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation current_timestamp_in_utc -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation type_timestamp in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-24-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation type_timestamp -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation end_of_all_times in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-25-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation end_of_all_times -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation generate_schema_name in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-26-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation generate_schema_name -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      
      # Run different tests for Oracle and every other database, because of different syntax in Oracle.
      - name: "Run run-operation get_query_results_as_dict --args '{query: select 1}' in ${{ inputs.db_name }} container"
        if: ${{ (success() || failure()) && inputs.db_name != 'oracle' }}
        run: |
          docker run --name dv4dbt-macro-test-27-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation get_query_results_as_dict --args '{query: select 1}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation get_query_results_as_dict --args '{query: select 1 from dual}' in ${{ inputs.db_name }} container"
        if: ${{ (success() || failure()) && inputs.db_name == 'oracle' }}
        run: |
          docker run --name dv4dbt-macro-test-27-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation get_query_results_as_dict --args '{query: select 1 from dual}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      
      - name: "Run run-operation get_standard_string --args '{string_list: abc}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-28-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation get_standard_string --args '{string_list: abc}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation ghost_record_per_datatype --args '{column_name: col, datatype: string, ghost_record_type: unknown}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-29-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation ghost_record_per_datatype --args '{column_name: col, datatype: string, ghost_record_type: unknown}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation hash_default_values in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-30-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation hash_default_values -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation hash_method in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-31-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation hash_method -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation attribute_standardise in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-32-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation attribute_standardise -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation hash --args '{columns: abc, alias: def}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-33-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation hash --args '{columns: abc, alias: def}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation limit_rows in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-34-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation limit_rows -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation max_datetime in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-35-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation max_datetime -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation prefix --args '{columns: abc, prefix_str: def}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-38-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation prefix --args '{columns: abc, prefix_str: def}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: "Run run-operation source_model_processing --args '{source_models: abc}' in ${{ inputs.db_name }} container"
        if: success() || failure()
        run: |
          docker run --name dv4dbt-macro-test-39-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation source_model_processing --args '{source_models: abc}' -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation string_to_timestamp in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-40-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation string_to_timestamp -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
      - name: Run run-operation timestamp_format in ${{ inputs.db_name }} container
        if: success() || failure()
        run: docker run --name dv4dbt-macro-test-41-${{ inputs.db_name }} --network host dv4dbt-${{ inputs.db_name }} dbt run-operation timestamp_format -d --profiles-dir /user/app/profiles --profile ${{ inputs.db_name }}
  
  # trigger the destruction of environments if set as a variable during call, after all tests are done.
  trigger-destroy-environments:
    if: ${{ always() && inputs.auto-destroy != 'false' }} 
    needs: [ dbt-tests, dbt-macro-tests ]

  # Same setup jobs as in the setup of the environments.
    defaults:
      run:
        working-directory: ./db-environments/${{ inputs.db_name }}

    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Configure AWS Credentials Action For GitHub Actions
      uses: aws-actions/configure-aws-credentials@v1-node16
      with:
        aws-access-key-id:  ${{ secrets.TFAUTOMATION_AWS_ACCESS_KEY }}
        aws-secret-access-key:  ${{ secrets.TFAUTOMATION_AWS_SECRET_ACCESS_KEY }}
        aws-region: eu-west-1

    - name: 'Az CLI login'
      if: ${{ inputs.db_name == 'synapse' || inputs.db_name == 'fabric'}}
      uses: azure/login@v1
      with:
        client-id: ${{ secrets.TFAUTOMATION_AZURE_CLIENT_ID}}
        tenant-id: ${{ secrets.TFAUTOMATION_AZURE_TENANT_ID}}
        subscription-id: ${{ secrets.TFAUTOMATION_AZURE_SUBSCRIPTION_ID }}

    - name: 'Authenticate to Google Cloud'
      if: ${{ inputs.db_name == 'bigquery' }}
      uses: 'google-github-actions/auth@v0.4.0'
      with:
        credentials_json: '${{ secrets.DV4DBT_SA_KEYS }}'

    - name: Create terraform.tfvars for synapse
      if: ${{ inputs.db_name == 'synapse' }}
      run: |
          cd $GITHUB_WORKSPACE/db-environments/synapse/ && cat << EOF > terraform.tfvars
          sql_administrator_login_password="${{ secrets.SYNAPSE_DB_PASSWORD }}"
          EOF

    - name: Create terraform.tfvars for databricks
      if: ${{ inputs.db_name == 'databricks' }}
      run: |
          cd $GITHUB_WORKSPACE/db-environments/databricks/ && cat << EOF > terraform.tfvars
          token="${{ secrets.DATABRICKS_TOKEN }}"
          host="${{ secrets.DATABRICKS_HOST}}"
          EOF
    - name: Create terraform.tfvars for snowflake
      if: ${{ inputs.db_name == 'snowflake' }}
      run: |
          cd $GITHUB_WORKSPACE/db-environments/snowflake/ && cat << EOF > snowflake_authenticator_key.pem
          ${{ secrets.SNOWFLAKE_PRIVATE_KEY }}
          EOF

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v2
      with:
        terraform_version: 1.8.0
    
    - name: Terraform Init
      run: terraform init -backend-config "key=datavault4dbtenvironments/${{ github.run_id }}/${{ inputs.db_name }}/terraform.state" -backend-config "bucket=scalefree-tf-backend" -backend-config "region=eu-west-1" -backend-config "dynamodb_table=scalefree-tf-lock-backend"
        
    # destruction of created terraform ressources.
    - name: Terraform Destroy
      run: terraform destroy -auto-approve -var="run_id=${{ github.run_id }}"

    # Create a token for curl API Call later. !!WORKFLOW_APP_FABRIC_SECRET expires after two years (15.11.2026) and needs to be renewed.
    - name: Get Access Token
      if: ${{ inputs.db_name == 'fabric' }}
      id: get_token
      run: |
        TOKEN_RESPONSE=$(curl -X POST -H "Content-Type: application/x-www-form-urlencoded" \
        -d "grant_type=client_credentials&client_id=${{ secrets.TFAUTOMATION_AZURE_CLIENT_ID }}&client_secret=${{ secrets.WORKFLOW_APP_FABRIC_SECRET }}&resource=https://management.azure.com/" \
        https://login.microsoftonline.com/${{ secrets.TFAUTOMATION_AZURE_TENANT_ID }}/oauth2/token)
        echo "ACCESS_TOKEN=$(echo $TOKEN_RESPONSE | jq -r .access_token)" >> $GITHUB_ENV

    - name: Decrement active count in DynamoDB and stop Capacity
      if: ${{ inputs.db_name == 'fabric' }}
      run: |
          # Decrement semaphore and get the new Value
          aws dynamodb update-item \
            --table-name dv4dbt-cicd-semaphore \
            --key '{"ID": {"S": "capacity"}}' \
            --update-expression "SET active_count = active_count - :decr" \
            --expression-attribute-values '{":decr": {"N": "1"}}' \
            --return-values "UPDATED_NEW" > output.json

          # Get new value of active_count (semaphore) 
          ACTIVE_COUNT=$(cat output.json | jq -r '.Attributes.active_count.N')

          # If semaphore = 0, capacity will be paused
          if [ "$ACTIVE_COUNT" -eq "0" ]; then
            echo "Pausing capacity as no active runs are left."
             curl -X POST \
              -H "Authorization: Bearer $ACCESS_TOKEN" \
              -H "Content-Type: application/json" \
              -H "Content-Length: 0" \
              "https://management.azure.com/subscriptions/${{ secrets.TFAUTOMATION_AZURE_SUBSCRIPTION_ID }}/resourceGroups/dv4dbt_db_environment/providers/Microsoft.Fabric/capacities/dv4dbtcicd/suspend?api-version=2023-11-01"
          else
            echo "Remaining active runs: $ACTIVE_COUNT. Capacity will not be paused."
          fi

    # Job to send the results of tests for better monitoring.
  collect-job-results:
    if: always()
    needs: [ dbt-tests, dbt-macro-tests ]

    runs-on: ubuntu-latest
    outputs: # defining outputs
      dbt-test-result: ${{ steps.results.outputs.dbt-test-result}}
      dbt-macro-test-result: ${{ steps.results.outputs.dbt-macro-test-result}}

    steps:
    - name: Collect results
      id: results
      # setting output values.
      run: |
        echo "dbt-test-result=${{ needs.dbt-tests.result }}" >> $GITHUB_OUTPUT
        echo "dbt-macro-test-result=${{ needs.dbt-macro-tests.result }}" >> $GITHUB_OUTPUT


